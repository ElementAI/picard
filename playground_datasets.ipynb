{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from datasets.load import load_dataset\n",
    "from transformers.training_args_seq2seq import Seq2SeqTrainingArguments\n",
    "from seq2seq.utils.args import ModelArguments\n",
    "from seq2seq.utils.picard_model_wrapper import PicardArguments\n",
    "from seq2seq.utils.dataset import DataTrainingArguments, DataArguments\n",
    "from transformers.hf_argparser import HfArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser(\n",
    "        (PicardArguments, ModelArguments, DataArguments, DataTrainingArguments, Seq2SeqTrainingArguments)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "picard_args, model_args, data_args, data_training_args, training_args = parser.parse_json_file('./configs/train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mlist\u001b[39m(x)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "list(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/transformers_cache'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataArguments(dataset='spider', dataset_paths={'spider': './seq2seq/datasets/spider', 'cosql': './seq2seq/datasets/cosql', 'spider_realistic': './seq2seq/datasets/spider_realistic', 'spider_syn': './seq2seq/datasets/spider_syn', 'spider_dk': './seq2seq/datasets/spider_dk'}, metric_config='both', metric_paths={'spider': './seq2seq/metrics/spider', 'spider_realistic': './seq2seq/metrics/spider', 'cosql': './seq2seq/metrics/cosql', 'spider_syn': './seq2seq/metrics/spider', 'spider_dk': './seq2seq/metrics/spider'}, test_suite_db_dir=None, data_config_file=None, test_sections=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset spider (./transformers_cache/spider/spider/1.0.0/2ce75fa75bce00ee54968cffd084f961fdb357e6d67f97567e433f61279d35bc)\n",
      "100%|██████████| 2/2 [00:00<00:00, 501.32it/s]\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\n",
    "        path='./seq2seq/datasets/spider', cache_dir=\"./transformers_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'SELECT count(DISTINCT temporary_acting) FROM management',\n",
       " 'question': 'How many acting statuses are there?',\n",
       " 'db_id': 'department_management',\n",
       " 'db_path': './transformers_cache/downloads/extracted/ed695757eccb4c1cb88cd0cf13a9574737c579f4914ed8cc993e08a8c22d4a2d/spider/database',\n",
       " 'db_table_names': ['department', 'head', 'management'],\n",
       " 'db_column_names': {'table_id': [-1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2],\n",
       "  'column_name': ['*',\n",
       "   'Department_ID',\n",
       "   'Name',\n",
       "   'Creation',\n",
       "   'Ranking',\n",
       "   'Budget_in_Billions',\n",
       "   'Num_Employees',\n",
       "   'head_ID',\n",
       "   'name',\n",
       "   'born_state',\n",
       "   'age',\n",
       "   'department_ID',\n",
       "   'head_ID',\n",
       "   'temporary_acting']},\n",
       " 'db_column_types': ['text',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text'],\n",
       " 'db_primary_keys': {'column_id': [1, 7, 11]},\n",
       " 'db_foreign_keys': {'column_id': [12, 11], 'other_column_id': [7, 1]}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset world_cup (./transformers_cache/world_cup/worldcup_v1/1.0.0/17610d2750dda45bbc86da66abbd89c7928f1de5aaa3a4558c71dcf5c3722d27)\n",
      "100%|██████████| 2/2 [00:00<00:00, 434.33it/s]\n",
      "Reusing dataset world_cup (./transformers_cache/world_cup/worldcup_v2/1.0.0/17610d2750dda45bbc86da66abbd89c7928f1de5aaa3a4558c71dcf5c3722d27)\n",
      "100%|██████████| 2/2 [00:00<00:00, 490.28it/s]\n",
      "Reusing dataset world_cup (./transformers_cache/world_cup/worldcup_v3/1.0.0/17610d2750dda45bbc86da66abbd89c7928f1de5aaa3a4558c71dcf5c3722d27)\n",
      "100%|██████████| 2/2 [00:00<00:00, 447.42it/s]\n"
     ]
    }
   ],
   "source": [
    "wc_1 = load_dataset('./seq2seq/datasets/worldcup', name='worldcup_v1', cache_dir=\"./transformers_cache\")\n",
    "wc_2 = load_dataset('./seq2seq/datasets/worldcup', name='worldcup_v2', cache_dir=\"./transformers_cache\")\n",
    "wc_3 = load_dataset('./seq2seq/datasets/worldcup', name='worldcup_v3', cache_dir=\"./transformers_cache\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query', 'question', 'db_id', 'db_uri', 'db_schema', 'db_table_names', 'db_column_names', 'db_column_types', 'db_primary_keys', 'db_foreign_keys'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['query', 'question', 'db_id', 'db_uri', 'db_schema', 'db_table_names', 'db_column_names', 'db_column_types', 'db_primary_keys', 'db_foreign_keys'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': \"SELECT Distinct T4.teamname FROM player as T1\\n join match_fact as T2 on T1.player_id = T2.player_id\\n join match AS T3 on T2.match_id = T3.match_id\\n join national_team AS T4 on T3.home_team_id = T4.team_id\\n WHERE T1.player_name ilike '%Giroud%' and T2.goal = 'true' and T4.team_id not in(\\n     select T5.team_id\\n     from player as T1\\n     join player_fact as T5 on T1.player_id = T5.player_id\\n     join national_team as T4 on T4.team_id = T5.team_id\\n     WHERE T1.player_name ilike '%Giroud%'\\n     )\\nUNION\\nSELECT Distinct T4.teamname FROM player as T1\\n join match_fact as T2 on T1.player_id = T2.player_id\\n join match AS T3 on T2.match_id = T3.match_id\\n join national_team AS T4 on T3.away_team_id = T4.team_id\\n WHERE T1.player_name ilike '%Giroud%' and T2.goal = 'true' and T4.team_id not in(\\n     select T5.team_id\\n     from player as T1\\n     join player_fact as T5 on T1.player_id = T5.player_id\\n     join national_team as T4 on T4.team_id = T5.team_id\\n     WHERE T1.player_name ilike '%Giroud%'\\n     )\",\n",
       " 'question': 'Against which team did Giroud shoot a goal?',\n",
       " 'db_id': 'exp_v1',\n",
       " 'db_uri': 'postgresql://inode_readonly:W8BYqhSemzyZ64YD@testbed.inode.igd.fraunhofer.de:18001/world_cup',\n",
       " 'db_schema': 'exp_v1',\n",
       " 'db_table_names': ['club',\n",
       "  'coach',\n",
       "  'league',\n",
       "  'national_team',\n",
       "  'player',\n",
       "  'stadium',\n",
       "  'club_league_history',\n",
       "  'coach_club_team',\n",
       "  'player_club_team',\n",
       "  'world_cup',\n",
       "  'match',\n",
       "  'player_fact',\n",
       "  'match_fact'],\n",
       " 'db_column_names': {'table_id': [-1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   5,\n",
       "   5,\n",
       "   5,\n",
       "   5,\n",
       "   5,\n",
       "   5,\n",
       "   6,\n",
       "   6,\n",
       "   6,\n",
       "   6,\n",
       "   7,\n",
       "   7,\n",
       "   7,\n",
       "   7,\n",
       "   7,\n",
       "   7,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   9,\n",
       "   9,\n",
       "   9,\n",
       "   9,\n",
       "   9,\n",
       "   9,\n",
       "   9,\n",
       "   9,\n",
       "   9,\n",
       "   9,\n",
       "   10,\n",
       "   10,\n",
       "   10,\n",
       "   10,\n",
       "   10,\n",
       "   10,\n",
       "   10,\n",
       "   10,\n",
       "   10,\n",
       "   10,\n",
       "   10,\n",
       "   10,\n",
       "   10,\n",
       "   10,\n",
       "   10,\n",
       "   10,\n",
       "   10,\n",
       "   10,\n",
       "   10,\n",
       "   11,\n",
       "   11,\n",
       "   11,\n",
       "   11,\n",
       "   11,\n",
       "   11,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12],\n",
       "  'column_name': ['*',\n",
       "   'club_id',\n",
       "   'club_name',\n",
       "   'country',\n",
       "   'found_year',\n",
       "   'nickname',\n",
       "   'coach_id',\n",
       "   'country_code',\n",
       "   'wikidata_id',\n",
       "   'coach_name',\n",
       "   'similarity',\n",
       "   'league_id',\n",
       "   'league_name',\n",
       "   'found_year',\n",
       "   'country',\n",
       "   'team_id',\n",
       "   'teamname',\n",
       "   'team_initials',\n",
       "   'goals',\n",
       "   'year',\n",
       "   'nickname',\n",
       "   'player_id',\n",
       "   'wikidata_id',\n",
       "   'player_name',\n",
       "   'similarity',\n",
       "   'dob',\n",
       "   'stadium_id',\n",
       "   'stadium_name',\n",
       "   'city',\n",
       "   'capacity',\n",
       "   'country',\n",
       "   'continent',\n",
       "   'club_id',\n",
       "   'league_id',\n",
       "   'start_year',\n",
       "   'end_year',\n",
       "   'coach_id',\n",
       "   'club_id',\n",
       "   'club_team_name',\n",
       "   'start_year',\n",
       "   'end_year',\n",
       "   'coach_wikidata_id',\n",
       "   'player_id',\n",
       "   'club_id',\n",
       "   'club_team_name',\n",
       "   'start_year',\n",
       "   'end_year',\n",
       "   'shirt_number',\n",
       "   'position',\n",
       "   'year',\n",
       "   'venue',\n",
       "   'goals_scored',\n",
       "   'qualified_team',\n",
       "   'matches_played',\n",
       "   'attendance',\n",
       "   'winner',\n",
       "   'runner_up',\n",
       "   'third',\n",
       "   'fourth',\n",
       "   'match_id',\n",
       "   'year',\n",
       "   'round_id',\n",
       "   'datetime',\n",
       "   'stage',\n",
       "   'win_conditions',\n",
       "   'referee',\n",
       "   'assistant_1',\n",
       "   'assistant_2',\n",
       "   'attendance',\n",
       "   'stadium_id',\n",
       "   'did_home_win',\n",
       "   'is_draw',\n",
       "   'home_team_id',\n",
       "   'away_team_id',\n",
       "   'half_time_home_goals',\n",
       "   'home_team_goals',\n",
       "   'half_time_away_team_goals',\n",
       "   'away_team_goals',\n",
       "   'coach_id',\n",
       "   'team_id',\n",
       "   'year_id',\n",
       "   'player_id',\n",
       "   'shirt_number',\n",
       "   'position',\n",
       "   'player_id',\n",
       "   'match_id',\n",
       "   'minute',\n",
       "   'goal',\n",
       "   'own_goal',\n",
       "   'yellow_card',\n",
       "   'red_card',\n",
       "   'second_yellow_card',\n",
       "   'penalty',\n",
       "   'missed_penalty',\n",
       "   'substitution_in',\n",
       "   'substitution_out',\n",
       "   'line_up',\n",
       "   'team_id']},\n",
       " 'db_column_types': ['text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'text',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'text',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number'],\n",
       " 'db_primary_keys': {'column_id': [1, 6, 11, 15, 21, 26, 49, 59]},\n",
       " 'db_foreign_keys': {'column_id': [32,\n",
       "   33,\n",
       "   36,\n",
       "   37,\n",
       "   42,\n",
       "   43,\n",
       "   55,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   60,\n",
       "   69,\n",
       "   72,\n",
       "   73,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   84,\n",
       "   85],\n",
       "  'other_column_id': [1,\n",
       "   11,\n",
       "   6,\n",
       "   1,\n",
       "   21,\n",
       "   1,\n",
       "   15,\n",
       "   15,\n",
       "   15,\n",
       "   15,\n",
       "   49,\n",
       "   26,\n",
       "   15,\n",
       "   15,\n",
       "   6,\n",
       "   15,\n",
       "   49,\n",
       "   21,\n",
       "   21,\n",
       "   59]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Any, Iterable, List, Optional\n",
    "\n",
    "from sqlalchemy import MetaData, create_engine, inspect, select, text, func\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.exc import ProgrammingError, SQLAlchemyError\n",
    "from sqlalchemy.schema import CreateTable\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import warnings\n",
    "from functools import lru_cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'club': {'COL': [['club_id', 'VARCHAR'],\n",
       "   ['club_name', 'VARCHAR'],\n",
       "   ['country', 'VARCHAR'],\n",
       "   ['found_year', 'INTEGER']],\n",
       "  'PK': ['club_id'],\n",
       "  'FK': []},\n",
       " 'coach': {'COL': [['nickname', 'VARCHAR'],\n",
       "   ['coach_id', 'INTEGER'],\n",
       "   ['country_code', 'VARCHAR'],\n",
       "   ['wikidata_id', 'VARCHAR'],\n",
       "   ['coach_name', 'VARCHAR'],\n",
       "   ['similarity', 'REAL']],\n",
       "  'PK': ['coach_id'],\n",
       "  'FK': []},\n",
       " 'league': {'COL': [['league_id', 'VARCHAR'],\n",
       "   ['league_name', 'VARCHAR'],\n",
       "   ['found_year', 'INTEGER'],\n",
       "   ['country', 'VARCHAR']],\n",
       "  'PK': ['league_id'],\n",
       "  'FK': []},\n",
       " 'national_team': {'COL': [['team_id', 'INTEGER'],\n",
       "   ['teamname', 'VARCHAR'],\n",
       "   ['team_initials', 'VARCHAR(3)'],\n",
       "   ['goals', 'INTEGER'],\n",
       "   ['year', 'INTEGER']],\n",
       "  'PK': ['team_id'],\n",
       "  'FK': []},\n",
       " 'player': {'COL': [['nickname', 'VARCHAR'],\n",
       "   ['player_id', 'INTEGER'],\n",
       "   ['wikidata_id', 'VARCHAR'],\n",
       "   ['player_name', 'VARCHAR'],\n",
       "   ['similarity', 'REAL'],\n",
       "   ['dob', 'TIMESTAMP']],\n",
       "  'PK': ['player_id'],\n",
       "  'FK': []},\n",
       " 'stadium': {'COL': [['stadium_id', 'INTEGER'],\n",
       "   ['stadium_name', 'VARCHAR'],\n",
       "   ['city', 'VARCHAR'],\n",
       "   ['capacity', 'INTEGER'],\n",
       "   ['country', 'VARCHAR'],\n",
       "   ['continent', 'VARCHAR']],\n",
       "  'PK': ['stadium_id'],\n",
       "  'FK': []},\n",
       " 'club_league_history': {'COL': [['club_id', 'VARCHAR'],\n",
       "   ['league_id', 'VARCHAR'],\n",
       "   ['start_year', 'INTEGER'],\n",
       "   ['end_year', 'INTEGER']],\n",
       "  'PK': [],\n",
       "  'FK': [['club_league_history.club_id', 'club.club_id'],\n",
       "   ['club_league_history.league_id', 'league.league_id']]},\n",
       " 'coach_club_team': {'COL': [['coach_id', 'INTEGER'],\n",
       "   ['club_id', 'VARCHAR'],\n",
       "   ['club_team_name', 'VARCHAR'],\n",
       "   ['start_year', 'INTEGER'],\n",
       "   ['end_year', 'INTEGER'],\n",
       "   ['coach_wikidata_id', 'VARCHAR']],\n",
       "  'PK': [],\n",
       "  'FK': [['coach_club_team.coach_id', 'coach.coach_id'],\n",
       "   ['coach_club_team.club_id', 'club.club_id']]},\n",
       " 'player_club_team': {'COL': [['player_id', 'INTEGER'],\n",
       "   ['club_id', 'VARCHAR'],\n",
       "   ['club_team_name', 'VARCHAR'],\n",
       "   ['start_year', 'INTEGER'],\n",
       "   ['end_year', 'INTEGER'],\n",
       "   ['shirt_number', 'INTEGER'],\n",
       "   ['position', 'INTEGER']],\n",
       "  'PK': [],\n",
       "  'FK': [['player_club_team.player_id', 'player.player_id'],\n",
       "   ['player_club_team.club_id', 'club.club_id']]},\n",
       " 'world_cup': {'COL': [['year', 'INTEGER'],\n",
       "   ['venue', 'VARCHAR'],\n",
       "   ['goals_scored', 'INTEGER'],\n",
       "   ['qualified_team', 'INTEGER'],\n",
       "   ['matches_played', 'INTEGER'],\n",
       "   ['attendance', 'INTEGER'],\n",
       "   ['winner', 'INTEGER'],\n",
       "   ['runner_up', 'INTEGER'],\n",
       "   ['third', 'INTEGER'],\n",
       "   ['fourth', 'INTEGER']],\n",
       "  'PK': ['year'],\n",
       "  'FK': [['world_cup.winner', 'national_team.team_id'],\n",
       "   ['world_cup.runner_up', 'national_team.team_id'],\n",
       "   ['world_cup.third', 'national_team.team_id'],\n",
       "   ['world_cup.fourth', 'national_team.team_id']]},\n",
       " 'match': {'COL': [['match_id', 'INTEGER'],\n",
       "   ['year', 'INTEGER'],\n",
       "   ['round_id', 'INTEGER'],\n",
       "   ['datetime', 'TIMESTAMP'],\n",
       "   ['stage', 'VARCHAR'],\n",
       "   ['win_conditions', 'VARCHAR'],\n",
       "   ['referee', 'VARCHAR'],\n",
       "   ['assistant_1', 'VARCHAR'],\n",
       "   ['assistant_2', 'VARCHAR'],\n",
       "   ['attendance', 'INTEGER'],\n",
       "   ['stadium_id', 'INTEGER'],\n",
       "   ['did_home_win', 'BOOLEAN'],\n",
       "   ['is_draw', 'BOOLEAN'],\n",
       "   ['home_team_id', 'INTEGER'],\n",
       "   ['away_team_id', 'INTEGER'],\n",
       "   ['half_time_home_goals', 'INTEGER'],\n",
       "   ['home_team_goals', 'INTEGER'],\n",
       "   ['half_time_away_team_goals', 'INTEGER'],\n",
       "   ['away_team_goals', 'INTEGER']],\n",
       "  'PK': ['match_id'],\n",
       "  'FK': [['match.year', 'world_cup.year'],\n",
       "   ['match.stadium_id', 'stadium.stadium_id'],\n",
       "   ['match.home_team_id', 'national_team.team_id'],\n",
       "   ['match.away_team_id', 'national_team.team_id']]},\n",
       " 'player_fact': {'COL': [['coach_id', 'INTEGER'],\n",
       "   ['team_id', 'INTEGER'],\n",
       "   ['year_id', 'INTEGER'],\n",
       "   ['player_id', 'INTEGER'],\n",
       "   ['shirt_number', 'INTEGER'],\n",
       "   ['position', 'VARCHAR']],\n",
       "  'PK': [],\n",
       "  'FK': [['player_fact.coach_id', 'coach.coach_id'],\n",
       "   ['player_fact.team_id', 'national_team.team_id'],\n",
       "   ['player_fact.year_id', 'world_cup.year'],\n",
       "   ['player_fact.player_id', 'player.player_id']]},\n",
       " 'match_fact': {'COL': [['player_id', 'INTEGER'],\n",
       "   ['match_id', 'INTEGER'],\n",
       "   ['minute', 'INTEGER'],\n",
       "   ['goal', 'BOOLEAN'],\n",
       "   ['own_goal', 'BOOLEAN'],\n",
       "   ['yellow_card', 'BOOLEAN'],\n",
       "   ['red_card', 'BOOLEAN'],\n",
       "   ['second_yellow_card', 'BOOLEAN'],\n",
       "   ['penalty', 'BOOLEAN'],\n",
       "   ['missed_penalty', 'BOOLEAN'],\n",
       "   ['substitution_in', 'BOOLEAN'],\n",
       "   ['substitution_out', 'BOOLEAN'],\n",
       "   ['line_up', 'BOOLEAN'],\n",
       "   ['team_id', 'INTEGER']],\n",
       "  'PK': [],\n",
       "  'FK': [['match_fact.player_id', 'player.player_id'],\n",
       "   ['match_fact.match_id', 'match.match_id']]}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SQLDatabase(object):\n",
    "    \"\"\"SQLAlchemy wrapper around a database.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        engine: Engine,\n",
    "        schema: Optional[str] = None,\n",
    "        metadata: Optional[MetaData] = None,\n",
    "        ignore_tables: Optional[List[str]] = None,\n",
    "        include_tables: Optional[List[str]] = None,\n",
    "        sample_rows_in_table_info: int = 3,\n",
    "        indexes_in_table_info: bool = False,\n",
    "        custom_table_info: Optional[dict] = None,\n",
    "        view_support: bool = False,\n",
    "        max_string_length: int = 300,\n",
    "    ):\n",
    "        \"\"\"Create engine from database URI.\"\"\"\n",
    "        self._engine = engine\n",
    "        self._schema = schema\n",
    "        if include_tables and ignore_tables:\n",
    "            raise ValueError(\n",
    "                \"Cannot specify both include_tables and ignore_tables\")\n",
    "\n",
    "        self._inspector = inspect(self._engine)\n",
    "        # including view support by adding the views as well as tables to the all\n",
    "        # tables list if view_support is True\n",
    "        self._all_tables = set(\n",
    "            self._inspector.get_table_names(schema=schema)\n",
    "            + (self._inspector.get_view_names(schema=schema) if view_support else [])\n",
    "        )\n",
    "        self._include_tables = set(include_tables) if include_tables else set()\n",
    "        if self._include_tables:\n",
    "            missing_tables = self._include_tables - self._all_tables\n",
    "            if missing_tables:\n",
    "                raise ValueError(\n",
    "                    f\"include_tables {missing_tables} not found in database\"\n",
    "                )\n",
    "        self._ignore_tables = set(ignore_tables) if ignore_tables else set()\n",
    "        if self._ignore_tables:\n",
    "            missing_tables = self._ignore_tables - self._all_tables\n",
    "            if missing_tables:\n",
    "                raise ValueError(\n",
    "                    f\"ignore_tables {missing_tables} not found in database\"\n",
    "                )\n",
    "        usable_tables = self.get_usable_table_names()\n",
    "        self._usable_tables = set(usable_tables) if usable_tables else self._all_tables\n",
    "\n",
    "        if not isinstance(sample_rows_in_table_info, int):\n",
    "            raise TypeError(\"sample_rows_in_table_info must be an integer\")\n",
    "\n",
    "        self._sample_rows_in_table_info = sample_rows_in_table_info\n",
    "        self._indexes_in_table_info = indexes_in_table_info\n",
    "\n",
    "        self._custom_table_info = custom_table_info\n",
    "        if self._custom_table_info:\n",
    "            if not isinstance(self._custom_table_info, dict):\n",
    "                raise TypeError(\n",
    "                    \"table_info must be a dictionary with table names as keys and the \"\n",
    "                    \"desired table info as values\"\n",
    "                )\n",
    "            # only keep the tables that are also present in the database\n",
    "            intersection = set(self._custom_table_info).intersection(self._all_tables)\n",
    "            self._custom_table_info = dict(\n",
    "                (table, self._custom_table_info[table])\n",
    "                for table in self._custom_table_info\n",
    "                if table in intersection\n",
    "            )\n",
    "\n",
    "        self._max_string_length = max_string_length\n",
    "        \n",
    "        self._metadata = metadata or MetaData()\n",
    "        # including view support if view_support = true\n",
    "        self._metadata.reflect(\n",
    "            views=view_support,\n",
    "            bind=self._engine,\n",
    "            only=list(self._usable_tables),\n",
    "            schema=self._schema,\n",
    "            )\n",
    "\n",
    "    @classmethod\n",
    "    def from_uri(\n",
    "        cls, database_uri: str, engine_args: Optional[dict] = None, **kwargs: Any\n",
    "    ) -> SQLDatabase:\n",
    "        \"\"\"Construct a SQLAlchemy engine from URI.\"\"\"\n",
    "        _engine_args = engine_args or {}\n",
    "        return cls(create_engine(database_uri, **_engine_args), **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_databricks(\n",
    "        cls,\n",
    "        catalog: str,\n",
    "        schema: str,\n",
    "        host: Optional[str] = None,\n",
    "        api_token: Optional[str] = None,\n",
    "        warehouse_id: Optional[str] = None,\n",
    "        cluster_id: Optional[str] = None,\n",
    "        engine_args: Optional[dict] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> SQLDatabase:\n",
    "        \"\"\"\n",
    "        Class method to create an SQLDatabase instance from a Databricks connection.\n",
    "        This method requires the 'databricks-sql-connector' package. If not installed,\n",
    "        it can be added using `pip install databricks-sql-connector`.\n",
    "\n",
    "        Args:\n",
    "            catalog (str): The catalog name in the Databricks database.\n",
    "            schema (str): The schema name in the catalog.\n",
    "            host (Optional[str]): The Databricks workspace hostname, excluding\n",
    "                'https://' part. If not provided, it attempts to fetch from the\n",
    "                environment variable 'DATABRICKS_HOST'. If still unavailable and if\n",
    "                running in a Databricks notebook, it defaults to the current workspace\n",
    "                hostname. Defaults to None.\n",
    "            api_token (Optional[str]): The Databricks personal access token for\n",
    "                accessing the Databricks SQL warehouse or the cluster. If not provided,\n",
    "                it attempts to fetch from 'DATABRICKS_TOKEN'. If still unavailable\n",
    "                and running in a Databricks notebook, a temporary token for the current\n",
    "                user is generated. Defaults to None.\n",
    "            warehouse_id (Optional[str]): The warehouse ID in the Databricks SQL. If\n",
    "                provided, the method configures the connection to use this warehouse.\n",
    "                Cannot be used with 'cluster_id'. Defaults to None.\n",
    "            cluster_id (Optional[str]): The cluster ID in the Databricks Runtime. If\n",
    "                provided, the method configures the connection to use this cluster.\n",
    "                Cannot be used with 'warehouse_id'. If running in a Databricks notebook\n",
    "                and both 'warehouse_id' and 'cluster_id' are None, it uses the ID of the\n",
    "                cluster the notebook is attached to. Defaults to None.\n",
    "            engine_args (Optional[dict]): The arguments to be used when connecting\n",
    "                Databricks. Defaults to None.\n",
    "            **kwargs (Any): Additional keyword arguments for the `from_uri` method.\n",
    "\n",
    "        Returns:\n",
    "            SQLDatabase: An instance of SQLDatabase configured with the provided\n",
    "                Databricks connection details.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If 'databricks-sql-connector' is not found, or if both\n",
    "                'warehouse_id' and 'cluster_id' are provided, or if neither\n",
    "                'warehouse_id' nor 'cluster_id' are provided and it's not executing\n",
    "                inside a Databricks notebook.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from databricks import sql  # noqa: F401\n",
    "        except ImportError:\n",
    "            raise ValueError(\n",
    "                \"databricks-sql-connector package not found, please install with\"\n",
    "                \" `pip install databricks-sql-connector`\"\n",
    "            )\n",
    "        context = None\n",
    "        try:\n",
    "            from dbruntime.databricks_repl_context import get_context\n",
    "\n",
    "            context = get_context()\n",
    "        except ImportError:\n",
    "            pass\n",
    "\n",
    "        default_host = context.browserHostName if context else None\n",
    "        if host is None:\n",
    "            host = tools.get_from_env(\"host\", \"DATABRICKS_HOST\", default_host)\n",
    "\n",
    "        default_api_token = context.apiToken if context else None\n",
    "        if api_token is None:\n",
    "            api_token = tools.get_from_env(\n",
    "                \"api_token\", \"DATABRICKS_TOKEN\", default_api_token\n",
    "            )\n",
    "\n",
    "        if warehouse_id is None and cluster_id is None:\n",
    "            if context:\n",
    "                cluster_id = context.clusterId\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Need to provide either 'warehouse_id' or 'cluster_id'.\"\n",
    "                )\n",
    "\n",
    "        if warehouse_id and cluster_id:\n",
    "            raise ValueError(\"Can't have both 'warehouse_id' or 'cluster_id'.\")\n",
    "\n",
    "        if warehouse_id:\n",
    "            http_path = f\"/sql/1.0/warehouses/{warehouse_id}\"\n",
    "        else:\n",
    "            http_path = f\"/sql/protocolv1/o/0/{cluster_id}\"\n",
    "\n",
    "        uri = (\n",
    "            f\"databricks://token:{api_token}@{host}?\"\n",
    "            f\"http_path={http_path}&catalog={catalog}&schema={schema}\"\n",
    "        )\n",
    "        return cls.from_uri(database_uri=uri, engine_args=engine_args, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def dialect(self) -> str:\n",
    "        \"\"\"Return string representation of dialect to use.\"\"\"\n",
    "        return self._engine.dialect.name\n",
    "\n",
    "    def get_usable_table_names(self) -> Iterable[str]:\n",
    "        \"\"\"Get names of tables available.\"\"\"\n",
    "        if self._include_tables:\n",
    "            return self._include_tables\n",
    "        return self._all_tables - self._ignore_tables\n",
    "    \n",
    "    def get_table_names(self) -> Iterable[str]:\n",
    "        \"\"\"Get names of tables available.\"\"\"\n",
    "        warnings.warn(\n",
    "            \"This method is deprecated - please use `get_usable_table_names`.\"\n",
    "        )\n",
    "        return self.get_usable_table_names()\n",
    "    \n",
    "    @property\n",
    "    def table_info(self) -> str:\n",
    "        \"\"\"Information about all tables in the database.\"\"\"\n",
    "        return self.get_table_info()\n",
    "\n",
    "    def get_table_info(self, table_names: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"Get information about specified tables.\n",
    "\n",
    "        Follows best practices as specified in: Rajkumar et al, 2022\n",
    "        (https://arxiv.org/abs/2204.00498)\n",
    "\n",
    "        If `sample_rows_in_table_info`, the specified number of sample rows will be\n",
    "        appended to each table description. This can increase performance as\n",
    "        demonstrated in the paper.\n",
    "        \"\"\"\n",
    "        all_table_names = self.get_usable_table_names()\n",
    "        if table_names is not None:\n",
    "            missing_tables = set(table_names).difference(all_table_names)\n",
    "            if missing_tables:\n",
    "                raise ValueError(f\"table_names {missing_tables} not found in database\")\n",
    "            all_table_names = table_names\n",
    "\n",
    "        meta_tables = [\n",
    "            tbl\n",
    "            for tbl in self._metadata.sorted_tables\n",
    "            if tbl.name in set(all_table_names)\n",
    "            and not (self.dialect == \"sqlite\" and tbl.name.startswith(\"sqlite_\"))\n",
    "        ]\n",
    "\n",
    "        tables = []\n",
    "        for table in meta_tables:\n",
    "            if self._custom_table_info and table.name in self._custom_table_info:\n",
    "                tables.append(self._custom_table_info[table.name])\n",
    "                continue\n",
    "\n",
    "            # add create table command\n",
    "            create_table = str(CreateTable(table).compile(self._engine))\n",
    "            table_info = f\"{create_table.rstrip()}\"\n",
    "            has_extra_info = (\n",
    "                self._indexes_in_table_info or self._sample_rows_in_table_info\n",
    "            )\n",
    "            if has_extra_info:\n",
    "                table_info += \"\\n\\n/*\"\n",
    "            if self._indexes_in_table_info:\n",
    "                table_info += f\"\\n{self._get_table_indexes(table)}\\n\"\n",
    "            if self._sample_rows_in_table_info:\n",
    "                table_info += f\"\\n{self._get_sample_rows(table)}\\n\"\n",
    "            if has_extra_info:\n",
    "                table_info += \"*/\"\n",
    "            tables.append(table_info)\n",
    "        final_str = \"\\n\\n\".join(tables)\n",
    "        return final_str\n",
    "    \n",
    "    def _get_table_indexes(self, table: Table) -> str:\n",
    "        indexes = self._inspector.get_indexes(table.name)\n",
    "        indexes_formatted = \"\\n\".join(map(_format_index, indexes))\n",
    "        return f\"Table Indexes:\\n{indexes_formatted}\"\n",
    "    \n",
    "    def _get_sample_rows(self, table: Table) -> str:\n",
    "\n",
    "        # build the select command\n",
    "        command = select(table).limit(self._sample_rows_in_table_info)\n",
    "\n",
    "        # save the command in string format\n",
    "        select_star = (\n",
    "            f\"SELECT * FROM '{table.name}' LIMIT \"\n",
    "            f\"{self._sample_rows_in_table_info}\"\n",
    "        )\n",
    "\n",
    "        # save the columns in string format\n",
    "        columns_str = \"\\t\".join([col.name for col in table.columns])\n",
    "\n",
    "        try:\n",
    "            # get the sample rows\n",
    "            with self._engine.connect() as connection:\n",
    "                try:\n",
    "                    sample_rows_result = connection.execute(command)\n",
    "                    # shorten values in the sample rows\n",
    "                \n",
    "                    sample_rows = list(\n",
    "                        map(lambda ls: [str(i)[:100]\n",
    "                            for i in ls], sample_rows_result)\n",
    "                    )\n",
    "                except TypeError as e:\n",
    "                    # print(\"***Back to literal querying...***\")\n",
    "                    sample_rows_result = connection.exec_driver_sql(select_star)\n",
    "                    # shorten values in the sample rows\n",
    "                    sample_rows = list(\n",
    "                        map(lambda ls: [str(i)[:100]\n",
    "                            for i in ls], sample_rows_result)\n",
    "                    )\n",
    "\n",
    "            # save the sample rows in string format\n",
    "            sample_rows_str = \"\\n\".join([\"\\t\".join(row) for row in sample_rows])\n",
    "        # in some dialects when there are no rows in the table a\n",
    "        # 'ProgrammingError' is returned\n",
    "        except ProgrammingError:\n",
    "            sample_rows_str = \"\"\n",
    "\n",
    "        return (\n",
    "            f\"{self._sample_rows_in_table_info} rows from {table.name} table:\\n\"\n",
    "            f\"{columns_str}\\n\"\n",
    "            f\"{sample_rows_str}\"\n",
    "        )\n",
    "\n",
    "    \n",
    "    def get_table_info_dict(self, table_names: Optional[List[str]] = None, with_col_details = True, do_sampling = True) -> dict:\n",
    "        all_table_names = self.get_usable_table_names()\n",
    "        if table_names is not None:\n",
    "            missing_tables = set(table_names).difference(all_table_names)\n",
    "            if missing_tables:\n",
    "                raise ValueError(f\"table_names {missing_tables} not found in database\")\n",
    "            all_table_names = table_names\n",
    "\n",
    "        meta_tables = [\n",
    "            tbl\n",
    "            for tbl in self._metadata.sorted_tables\n",
    "            if tbl.name in set(all_table_names)\n",
    "            and not (self.dialect == \"sqlite\" and tbl.name.startswith(\"sqlite_\"))\n",
    "        ]\n",
    "\n",
    "        tables = []\n",
    "        \"\"\"\n",
    "        all_table_names = self.get_table_names()\n",
    "        if table_names is not None:\n",
    "            missing_tables = set(table_names).difference(all_table_names)\n",
    "            if missing_tables:\n",
    "                raise ValueError(\n",
    "                    f\"table_names {missing_tables} not found in database\")\n",
    "            all_table_names = table_names\n",
    "\n",
    "        meta_tables = [\n",
    "            tbl\n",
    "            for tbl in self._metadata.sorted_tables\n",
    "            if tbl.name in set(all_table_names)\n",
    "            and not (self.dialect == \"sqlite\" and tbl.name.startswith(\"sqlite_\"))\n",
    "        ]\n",
    "\n",
    "        tables = []\n",
    "        \"\"\"\n",
    "\n",
    "        for table in meta_tables:\n",
    "            if self._custom_table_info and table.name in self._custom_table_info:\n",
    "                tables.append(self._custom_table_info[table.name])\n",
    "            else:\n",
    "                tables.append(table)\n",
    "        tables_dict = {}\n",
    "        for table in tables:\n",
    "            cols = []\n",
    "            col_details = []\n",
    "            pk = []\n",
    "            fks = []\n",
    "            sample_rows = []\n",
    "            num_rows = 0\n",
    "            if do_sampling:\n",
    "                sample_rows = self.get_tbl_samples_dict(table)\n",
    "                num_rows = self.get_rows_of_a_table(table)\n",
    "            for col in table.columns:\n",
    "                cols.append([col.name, str(col.type).split('.')[-1]])\n",
    "                if col.primary_key:\n",
    "                    pk.append(col.name)\n",
    "                if len(col.foreign_keys) > 0:\n",
    "                    for fk in list(col.foreign_keys):\n",
    "                        fks.append([f'{table.name}.{col.name}', '.'.join(fk.target_fullname.split('.')[-2:])])\n",
    "                        \n",
    "                if with_col_details and num_rows > 0:\n",
    "                    distinct_values = self.count_distinct_values_of_a_col(table, col)\n",
    "                    cardinality = len(distinct_values) / num_rows\n",
    "                    # here we use 3 simple conditions to filterout the categorical values:\n",
    "                    # 1. cardinality < 0.3\n",
    "                    # 2. total len(distinct_values) < 20\n",
    "                    # 3. '_id' not in name or name is not equal to 'id'\n",
    "                    if cardinality < 0.5 and len(distinct_values) < 20 and ('_id' not in col.name.lower() or col.name.lower() == 'id'): # maybe a categorical value\n",
    "                        col_details.append({'is_categorical': True, 'cardinality': cardinality, 'distinct_values': distinct_values})\n",
    "                    else:\n",
    "                        col_details.append({'is_categorical': False, 'cardinality': cardinality, 'distinct_values': distinct_values[:20]})\n",
    "            \n",
    "            tables_dict[table.name] = {\n",
    "                'COL': cols,\n",
    "                'PK': pk,\n",
    "                'FK': fks\n",
    "            }\n",
    "            if do_sampling:\n",
    "                tables_dict[table.name]['sample_rows'] = sample_rows\n",
    "            if with_col_details:\n",
    "                tables_dict[table.name]['COL_DETAILS'] = col_details \n",
    "        return tables_dict\n",
    "    \n",
    "    def get_tbl_samples_dict(self, table):\n",
    "        sample_rows_dict = {}\n",
    "        if self._sample_rows_in_table_info:\n",
    "            # build the select command\n",
    "            command = select(table).limit(self._sample_rows_in_table_info)\n",
    "\n",
    "            # save the command in string format\n",
    "            select_star = (\n",
    "                f\"SELECT * FROM '{table.name}' LIMIT \"\n",
    "                f\"{self._sample_rows_in_table_info}\"\n",
    "            )\n",
    "\n",
    "            # save the columns\n",
    "            columns = [col.name for col in table.columns]\n",
    "\n",
    "            # get the sample rows\n",
    "            try:\n",
    "                with self._engine.connect() as connection:\n",
    "                    try:\n",
    "                        sample_rows = connection.execute(command)\n",
    "                        # shorten values in the sample rows\n",
    "                    \n",
    "                        sample_rows = list(\n",
    "                            map(lambda ls: [str(i)[:100]\n",
    "                                for i in ls], sample_rows)\n",
    "                        )\n",
    "                    except TypeError as e:\n",
    "                        # print(\"***Back to literal querying...***\")\n",
    "                        sample_rows = connection.exec_driver_sql(select_star)\n",
    "                        # shorten values in the sample rows\n",
    "                        sample_rows = list(\n",
    "                            map(lambda ls: [str(i)[:100]\n",
    "                                for i in ls], sample_rows)\n",
    "                        )\n",
    "                sample_rows_T = list(map(list, zip(*sample_rows)))\n",
    "                for col, rows in zip(columns, sample_rows_T):\n",
    "                    sample_rows_dict[col] = rows\n",
    "            except ProgrammingError:\n",
    "                print('Warning: sampling error')\n",
    "                sample_rows_dict = {}\n",
    "        return sample_rows_dict\n",
    "\n",
    "    def get_rows_of_a_table(self, table):\n",
    "        command = select(func.count()).select_from(table)\n",
    "        try:\n",
    "            with self._engine.connect() as connection:\n",
    "                num_rows = connection.execute(command)\n",
    "                # print(table.name)\n",
    "                return num_rows.scalar()\n",
    "        except ProgrammingError:\n",
    "                warnings.warn('Fetching categorical values error')\n",
    "                return None\n",
    "    \n",
    "    def count_distinct_values_of_a_col(self, table, column, num_limit=100):\n",
    "        command = select(func.count(column), column).group_by(column).order_by(func.count(column).desc()).limit(num_limit)\n",
    "        try:\n",
    "            with self._engine.connect() as connection:\n",
    "                try:\n",
    "                    sample_rows = connection.execute(command).fetchall()\n",
    "                    # print(table.name, column.name)\n",
    "                    return [list(r) for r in sample_rows]\n",
    "                except ValueError as e:\n",
    "                    print(f\"ValueError: {e.__traceback__}\")\n",
    "                    # backdraw to use exec_driver_sql method\n",
    "                    select_str = (\n",
    "                        f\"SELECT COUNT(*) FROM '{table.name}' GROUP BY {column.name} ORDER BY COUNT(*) LIMIT {num_limit}\")\n",
    "                    sample_rows = connection.exec_driver_sql(select_str).fetchall()\n",
    "                    return [list(r) for r in sample_rows]\n",
    "        except ProgrammingError:\n",
    "                print('Warning: categorical error')\n",
    "                return []\n",
    "    \n",
    "    def run(self, command: str, fetch: str = \"all\", fmt = \"str\") -> str:\n",
    "        \"\"\"Execute a SQL command and return a string representing the results.\n",
    "\n",
    "        If the statement returns rows, a string of the results is returned.\n",
    "        If the statement returns no rows, an empty string is returned.\n",
    "        \"\"\"\n",
    "        with self._engine.begin() as connection:\n",
    "            if self._schema is not None:\n",
    "                if self.dialect == \"snowflake\":\n",
    "                    connection.exec_driver_sql(\n",
    "                        f\"ALTER SESSION SET search_path='{self._schema}'\"\n",
    "                    )\n",
    "                elif self.dialect == \"bigquery\":\n",
    "                    connection.exec_driver_sql(f\"SET @@dataset_id='{self._schema}'\")\n",
    "                else:\n",
    "                    connection.exec_driver_sql(f\"SET search_path TO {self._schema}\")\n",
    "            cursor = connection.execute(text(command))\n",
    "            if cursor.returns_rows:\n",
    "                if fetch == \"all\":\n",
    "                    result = cursor.fetchall()\n",
    "                elif fetch == \"one\":\n",
    "                    result = cursor.fetchone()[0]\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        \"Fetch parameter must be either 'one' or 'all'\")\n",
    "                if fmt == \"str\":\n",
    "                    return str(result)\n",
    "                elif fmt == \"list\":\n",
    "                    return list(result)\n",
    "        return \"\"\n",
    "\n",
    "    def get_table_info_no_throw(self, table_names: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"Get information about specified tables.\n",
    "\n",
    "        Follows best practices as specified in: Rajkumar et al, 2022\n",
    "        (https://arxiv.org/abs/2204.00498)\n",
    "\n",
    "        If `sample_rows_in_table_info`, the specified number of sample rows will be\n",
    "        appended to each table description. This can increase performance as\n",
    "        demonstrated in the paper.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.get_table_info(table_names)\n",
    "        except ValueError as e:\n",
    "            \"\"\"Format the error message\"\"\"\n",
    "            return f\"Error: {e}\"\n",
    "\n",
    "    def run_no_throw(self, command: str, fetch: str = \"all\") -> str:\n",
    "        \"\"\"Execute a SQL command and return a string representing the results.\n",
    "\n",
    "        If the statement returns rows, a string of the results is returned.\n",
    "        If the statement returns no rows, an empty string is returned.\n",
    "\n",
    "        If the statement throws an error, the error message is returned.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.run(command, fetch)\n",
    "        except SQLAlchemyError as e:\n",
    "            \"\"\"Format the error message\"\"\"\n",
    "            return f\"Error: {e}\"\n",
    "        \n",
    "    def dict2str(self, d):\n",
    "        text = []\n",
    "        for t,v in d.items():\n",
    "            _tbl = f'{t}:'\n",
    "            cols = []\n",
    "            pks = ['PK:']\n",
    "            fks = ['FK:']\n",
    "            for col in v['COL']:\n",
    "                cols.append(f'{col[0]}:{self.aliastype(col[1])}')\n",
    "            for pk in v['PK']:\n",
    "                pks.append(pk)\n",
    "            for fk in v['FK']:\n",
    "                fks.append('='.join(list(fk)))\n",
    "            \n",
    "            tbl = '\\n'.join([_tbl, ', '.join(cols), ' '.join(pks), ' '.join(fks)])\n",
    "            text.append(tbl)\n",
    "        return '\\n'.join(text)\n",
    "        \n",
    "    def aliastype(self, t):\n",
    "        _t = t[:3].lower()\n",
    "        if _t in ['int', 'tin', 'sma', 'med', 'big','uns', 'rea', 'dou', 'num', 'dec', 'tim']:\n",
    "            res = 'N' # numerical value\n",
    "        elif _t in ['tex', 'var', 'cha', 'nch', 'nat', 'nva', 'clo']:\n",
    "            res = 'T' # text value\n",
    "        elif _t in ['boo']:\n",
    "            res = 'B'\n",
    "        elif _t in ['dat']:\n",
    "            res = 'D'\n",
    "        else:\n",
    "            raise ValueError('Unsupported data type')\n",
    "        return res\n",
    "    \n",
    "    @lru_cache(maxsize=1000)\n",
    "    def cached_query_results(self, query, limit_num=100):\n",
    "        session = self.Session()\n",
    "        try:\n",
    "            result = session.execute(query).fetchmany(limit_num)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing query: {query}. Error: {str(e)}\")\n",
    "            raise\n",
    "        finally:\n",
    "            session.close()\n",
    "host = 'testbed.inode.igd.fraunhofer.de'\n",
    "port = 18001\n",
    "database = 'world_cup'\n",
    "username = 'inode_readonly'\n",
    "password = 'W8BYqhSemzyZ64YD'\n",
    "database_uri = f'postgresql://{username}:{password}@{host}:{str(port)}/{database}'\n",
    "db = SQLDatabase.from_uri(database_uri, schema='exp_v1')\n",
    "db.get_table_info_dict(do_sampling=False, with_col_details=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'testbed.inode.igd.fraunhofer.de'\n",
    "port = 18001\n",
    "database = 'world_cup'\n",
    "username = 'inode_readonly'\n",
    "password = 'W8BYqhSemzyZ64YD'\n",
    "database_uri = f'postgresql://{username}:{password}@{host}:{str(port)}/{database}'\n",
    "db = SQLDatabase.from_uri(database_uri, schema='exp_v1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m db\u001b[39m.\u001b[39;49mget_table_info_dict(do_sampling\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, with_col_details\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[20], line 368\u001b[0m, in \u001b[0;36mSQLDatabase.get_table_info_dict\u001b[0;34m(self, table_names, with_col_details, do_sampling)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(col\u001b[39m.\u001b[39mforeign_keys) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    367\u001b[0m     \u001b[39mfor\u001b[39;00m fk \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(col\u001b[39m.\u001b[39mforeign_keys):\n\u001b[0;32m--> 368\u001b[0m         fks\u001b[39m.\u001b[39mappend([\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mtable\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mcol\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(fk\u001b[39m.\u001b[39;49mtarget_fullname\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])])\n\u001b[1;32m    370\u001b[0m \u001b[39mif\u001b[39;00m with_col_details \u001b[39mand\u001b[39;00m num_rows \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    371\u001b[0m     distinct_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcount_distinct_values_of_a_col(table, col)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "db.get_table_info_dict(do_sampling=False, with_col_details=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<method-wrapper '__call__' of functools._lru_cache_wrapper object at 0x7f497b327670>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@lru_cache(maxsize=4)\n",
    "def a(b):\n",
    "    return b*2\n",
    "a.__call__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CacheInfo(hits=12, misses=44, maxsize=2, currsize=2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for b in [1,2,3,4]*10:\n",
    "    a(b)\n",
    "a.cache_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CacheInfo(hits=36, misses=4, maxsize=4, currsize=4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for b in [1,2,3,4]*10:\n",
    "    a(b)\n",
    "a.cache_info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
